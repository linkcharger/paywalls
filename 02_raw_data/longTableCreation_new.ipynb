{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel -u aschade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from os import makedirs\n",
    "from os.path import exists\n",
    "from shutil import rmtree # removes folder with everything in it\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")    # to suppress the dask metadata warnings (meta arg seems to be broken, not happy with anything i give)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import logging as log\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as matdates\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 150\n",
    "pd.options.display.max_columns = 50\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "def floor(test, limit):\n",
    "    return limit if test < limit else test\n",
    "\n",
    "def ceiling(test, limit):\n",
    "    return limit if test > limit else test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = SLURMCluster(\n",
    "    cores=40,                          \n",
    "    memory='500GB',  \n",
    "    \n",
    "    local_directory='~/scratch',\n",
    "    job_extra=[\n",
    "        '--time=10:00:00',\n",
    "        \n",
    "        '--partition=haswell',    \n",
    "        '--nodes=1',\n",
    "        \n",
    "        '--job-name=dask',\n",
    "        '--output=dask.out', \n",
    "        '--error=dask.error', \n",
    "        '--mail-user=aaron.schade@upf.edu',\n",
    "        '--mail-type=NONE', \n",
    "    ],    \n",
    "    n_workers=1,                 # this is internal to one job? one node? \n",
    "    \n",
    "    interface='ib0',               # workers, no diag: em1, em2, ib0,   # no workers: lo, em1.851, idrac, em3 & em4 (no ipv4)\n",
    "    scheduler_options={\n",
    "#         'interface': 'em1',      # it wont allow you specify both an interface AND a host address\n",
    "        'host': '10.30.50.163',    # launch on this address, open dashboard on the other?\n",
    "    },\n",
    ")\n",
    "cluster.scale(jobs=1)\n",
    "\n",
    "\n",
    "scheduler = Client(cluster)\n",
    "print(scheduler)\n",
    "dashboardLink = scheduler.dashboard_link.replace('10.30.50.163', '10.60.110.163')\n",
    "# dashboardLink = scheduler.dashboard_link\n",
    "print(dashboardLink)\n",
    "print(dashboardLink.replace('status', 'workers'))\n",
    "print(dashboardLink.replace('status', 'graph'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!squeue -u aschade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paywallsDF = pd.read_excel('paywalls.xlsx')\n",
    "paywallsDF = paywallsDF[['paywall_date', 'url']]\n",
    "\n",
    "paywalls = {}\n",
    "for index, row in paywallsDF[:18].iterrows():\n",
    "        paywalls[row['url']] = str(row['paywall_date'].date())\n",
    "\n",
    "paywalls['nytimes.com-1'] = '2012-04-01'\n",
    "paywalls['nytimes.com-2'] = '2017-12-01'\n",
    "\n",
    "print('paywalls:')\n",
    "pprint(paywalls, indent=4)\n",
    "\n",
    "######################################\n",
    "\n",
    "with open('newsSitesList.txt', 'r') as f:\n",
    "    newsSitesList = [line.strip() for line in f.readlines()]\n",
    "\n",
    "for site in paywalls.keys():\n",
    "    if site not in newsSitesList:\n",
    "        newsSitesList.append(site) \n",
    "       \n",
    "print('\\nnews sites:')\n",
    "pprint(newsSitesList[:10], indent=4)\n",
    "\n",
    "######################################\n",
    "\n",
    "colsOfInterest = [\n",
    "    # 'ref_domain_name', \n",
    "    'domain_name',\n",
    "    'event_date',\n",
    "\n",
    "    'pages_viewed', \n",
    "    'duration', \n",
    "    # 'event_time', \n",
    "\n",
    "    'hoh_most_education', \n",
    "    'census_region', \n",
    "    'household_size',\n",
    "    'hoh_oldest_age', \n",
    "    'household_income', \n",
    "    'children', \n",
    "    'racial_background',\n",
    "    'connection_speed', \n",
    "    'country_of_origin', \n",
    "    'zip_code', \n",
    "    ]\n",
    "###################\n",
    "\n",
    "individualCharacteristics = [\n",
    "    'hoh_most_education', \n",
    "    'census_region', \n",
    "    'household_size',\n",
    "    'hoh_oldest_age', \n",
    "    'household_income', \n",
    "    'children', \n",
    "    'racial_background',\n",
    "    'connection_speed', \n",
    "    'country_of_origin', \n",
    "    'zip_code', \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for newspaperSite, paywallDate in paywalls.items():\n",
    "    top10 = [\n",
    "#         'latimes.com', \n",
    "#         'nypost.com', \n",
    "#         'bostonglobe.com', \n",
    "#         'chicagotribune.com', \n",
    "#         'startribune.com', \n",
    "#         'newsday.com', \n",
    "        'washingtonpost.com', \n",
    "        'inquirer.com', \n",
    "        'tampabay.com', \n",
    "        'sfchronicle.com',\n",
    "    ]\n",
    "    if newspaperSite not in top10: continue\n",
    "        \n",
    "    year = int(paywallDate[:4])\n",
    "\n",
    "    ######################################################################################################\n",
    "\n",
    "    print(f'\\n' + f' {newspaperSite} '.center(80, '#'))\n",
    "\n",
    "    prePWmonths = pd.Timestamp(paywallDate).dayofyear/30.5\n",
    "    postPWmonths = 12 - prePWmonths\n",
    "    if prePWmonths < 1 or postPWmonths < 1: continue\n",
    "\n",
    "    idealRangeStart = pd.Timestamp(paywallDate) - pd.Timedelta('90 days')\n",
    "    idealRangeEnd = pd.Timestamp(paywallDate) + pd.Timedelta('90 days')\n",
    "    yearStart = pd.Timestamp(f'{year}-01-01')\n",
    "    yearEnd = pd.Timestamp(f'{year}-12-31')\n",
    "\n",
    "    rangeStart = floor(idealRangeStart, yearStart)\n",
    "    rangeEnd = ceiling(idealRangeEnd, yearEnd)\n",
    "    datesOfInterest = pd.date_range(rangeStart, rangeEnd, freq='W')\n",
    "\n",
    "    print(f'paywall date: {paywallDate}')\n",
    "    print(list(datesOfInterest.month.unique()))\n",
    "\n",
    "    ######################################################################################################\n",
    "\n",
    "    ddf = dd.read_parquet(\n",
    "        f'../comscore/parquet/{year}', \n",
    "        index='machine_id', \n",
    "        columns=colsOfInterest,\n",
    "        engine='fastparquet',   # you HAVE TO use the same engine to read as you did for creating the parquet files!!!  only then will fast index lookups work - however, you need the pyarrow (or python-snappy) package installed for google's amazing 'snappy' compression algo\n",
    "        )\n",
    "    ddf = ddf[ddf['event_date'].between(rangeStart, rangeEnd)]\n",
    "\n",
    "    visitors = list(\n",
    "        ddf.loc[ddf['domain_name'] == newspaperSite.strip('-12')]\n",
    "        .index\n",
    "        .unique()\n",
    "        .compute()\n",
    "        )\n",
    "\n",
    "    \n",
    "    resultsList = []\n",
    "    \n",
    "\n",
    "    for visitor in tqdm(visitors, desc='visitors done: '):\n",
    "        \n",
    "        # set index for later resampling\n",
    "        temp = ddf.loc[visitor].set_index('event_date')\n",
    "        temp['numberVisits'] = 1           # fill all rows with 1 -> when aggregated, becomes counter\n",
    "\n",
    "        # get all websites they visited\n",
    "        allSites = temp['domain_name'].unique()\n",
    "\n",
    "        \n",
    "        \n",
    "        ######### create big ddf with all combinations of dates and sites for this user #########\n",
    "        df1 = pd.DataFrame({'event_date': datesOfInterest})\n",
    "        df2 = pd.DataFrame({'domain_name': allSites})\n",
    "        df = pd.merge(df1, df2, how='cross')\n",
    "        thisVisitorDF = dd.from_pandas(pd.DataFrame(df), chunksize=100_000)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        ######### fill in date stuff, default variables, demographics #########\n",
    "        thisVisitorDF = thisVisitorDF.assign(\n",
    "            machine_id =           visitor, \n",
    "            day_of_month =         thisVisitorDF.event_date.dt.day, \n",
    "            week_of_month =        (thisVisitorDF.event_date.dt.day - 1)//7 + 1, \n",
    "            month =                thisVisitorDF.event_date.dt.month, \n",
    "            year =                 thisVisitorDF.event_date.dt.year, \n",
    "\n",
    "            news_site_dummy =      thisVisitorDF.domain_name.isin(newsSitesList)\n",
    "        )\n",
    "        \n",
    "#         demographics = temp[individualCharacteristics].loc[temp.index.min().compute()]\n",
    "#         demographics = temp[individualCharacteristics].head(1, compute=False).map_partitions(lambda df: df.to_dict())\n",
    "        demographics = temp[individualCharacteristics].head(1).squeeze().to_dict()\n",
    "        for characteristic in individualCharacteristics:\n",
    "            thisVisitorDF[characteristic] = demographics[characteristic]\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        ######### aggregate by day and website, do not keep individual characteristics #########\n",
    "        notIndividualCharacteristics = [col for col in temp.columns if col not in individualCharacteristics]\n",
    "        if 'event_date' in notIndividualCharacteristics: notIndividualCharacteristics.remove('event_date')      # since now in index\n",
    "#         metadata = {\n",
    "#              'domain_name':   'str',\n",
    "#              'event_date':    'datetime64[ns]',\n",
    "#              'pages_viewed':  'int',\n",
    "#              'duration':      'int',\n",
    "#              'number_visits': 'int'}\n",
    "#         meta = pd.DataFrame({\n",
    "#              'domain_name':   ['str'],\n",
    "#         #      'event_date':    pd.Timestamp('202datetime64[ns]',\n",
    "#              'pages_viewed':  [3],\n",
    "#              'duration':      [3],\n",
    "#              'number_visits': [3],\n",
    "#         })\n",
    "\n",
    "        agged = (\n",
    "            temp[notIndividualCharacteristics]\n",
    "            .groupby('domain_name')\n",
    "            .apply(lambda group: group.resample('W').sum())#, meta=meta)\n",
    "            .reset_index(drop=False)\n",
    "            )\n",
    "        \n",
    "        ######### merge and fill with zeroes #########\n",
    "        thisVisitorDF = thisVisitorDF.merge(agged, how='left', on=['domain_name', 'event_date'])\n",
    "        thisVisitorDF[['pages_viewed', 'duration', 'numberVisits']] = thisVisitorDF[['pages_viewed', 'duration', 'numberVisits']].fillna(0)\n",
    "\n",
    "        resultsList.append(thisVisitorDF)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    results = dd.multi.concat(resultsList)\n",
    "    results.to_csv(f'outputs/longTable/{newspaperSite}/long_table.csv', single_file=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/gpfs42/robbyfs/scratch/lab_rdurante/aschade/paywalls/outputs/longTable/nytimes.com-1/long_table.csv']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newspaperSite = 'nytimes.com-1'\n",
    "subfolder = f'outputs/longTable/{newspaperSite}/individuals'\n",
    "ddf = dd.read_csv(\n",
    "        f'{subfolder}/*.csv', \n",
    "        encoding_errors='replace', \n",
    ")\n",
    "\n",
    "\n",
    "ddf.to_csv(f'outputs/longTable/{newspaperSite}/long_table.csv', single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machine_id</th>\n",
       "      <th>date</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>domain_name</th>\n",
       "      <th>news_site_dummy</th>\n",
       "      <th>number_visits</th>\n",
       "      <th>number_pages_viewed</th>\n",
       "      <th>time_spent_on_site</th>\n",
       "      <th>hoh_most_education</th>\n",
       "      <th>census_region</th>\n",
       "      <th>household_size</th>\n",
       "      <th>hoh_oldest_age</th>\n",
       "      <th>household_income</th>\n",
       "      <th>children</th>\n",
       "      <th>racial_background</th>\n",
       "      <th>connection_speed</th>\n",
       "      <th>country_of_origin</th>\n",
       "      <th>zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99534294</td>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>envybox.io</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99534294</td>\n",
       "      <td>2017-10-15</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>dom.co.il</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99534294</td>\n",
       "      <td>2017-10-15</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>graniru.org</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99534294</td>\n",
       "      <td>2017-10-15</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>yandex.net</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99534294</td>\n",
       "      <td>2017-10-15</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>likebtn.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   machine_id        date  day_of_month  week_of_month  month  year  \\\n",
       "0    99534294  2017-09-10            10              2      9  2017   \n",
       "1    99534294  2017-10-15            15              3     10  2017   \n",
       "2    99534294  2017-10-15            15              3     10  2017   \n",
       "3    99534294  2017-10-15            15              3     10  2017   \n",
       "4    99534294  2017-10-15            15              3     10  2017   \n",
       "\n",
       "   domain_name  news_site_dummy  number_visits  number_pages_viewed  \\\n",
       "0   envybox.io                0              1                    1   \n",
       "1    dom.co.il                0              1                    1   \n",
       "2  graniru.org                0              0                    0   \n",
       "3   yandex.net                0              1                    1   \n",
       "4  likebtn.com                0              1                    1   \n",
       "\n",
       "   time_spent_on_site  hoh_most_education  census_region  household_size  \\\n",
       "0                   1                  99              3               1   \n",
       "1                   1                  99              3               1   \n",
       "2                   0                  99              3               1   \n",
       "3                   1                  99              3               1   \n",
       "4                   1                  99              3               1   \n",
       "\n",
       "   hoh_oldest_age  household_income  children  racial_background  \\\n",
       "0              11                16         0                  1   \n",
       "1              11                16         0                  1   \n",
       "2              11                16         0                  1   \n",
       "3              11                16         0                  1   \n",
       "4              11                16         0                  1   \n",
       "\n",
       "   connection_speed  country_of_origin  zip_code  \n",
       "0                 1                  0     77084  \n",
       "1                 1                  0     77084  \n",
       "2                 1                  0     77084  \n",
       "3                 1                  0     77084  \n",
       "4                 1                  0     77084  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################## testing ####################################\n",
    "\n",
    "ddf = dd.read_csv('outputs/longTable/nytimes.com-2/long_table.csv')\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 21 entries, machine_id to zip_code\n",
      "dtypes: object(2), int64(19)"
     ]
    }
   ],
   "source": [
    "ddf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49370796"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8379"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.machine_id.nunique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        44302037\n",
       "1         1825574\n",
       "2          855840\n",
       "3          415894\n",
       "4          286410\n",
       "           ...   \n",
       "3049            1\n",
       "3045            1\n",
       "3043            1\n",
       "3042            1\n",
       "35466           1\n",
       "Name: number_pages_viewed, Length: 3596, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.number_pages_viewed.value_counts().compute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
