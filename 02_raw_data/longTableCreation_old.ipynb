{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel -u aschade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/users/aschade/.local/lib/python3.8/site-packages/dask_jobqueue/core.py:20: FutureWarning: tmpfile is deprecated and will be removed in a future release. Please use dask.utils.tmpfile instead.\n",
      "  from distributed.utils import tmpfile\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "from os import makedirs\n",
    "from os.path import exists\n",
    "from pprint import pprint\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import logging as log\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as matdates\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 150\n",
    "pd.options.display.max_columns = 50\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "def floor(test, limit):\n",
    "    return limit if test < limit else test\n",
    "\n",
    "def ceiling(test, limit):\n",
    "    return limit if test > limit else test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Client: 'tcp://10.30.50.163:32788' processes=0 threads=0, memory=0 B>\n",
      "http://10.60.110.163:8787/status\n",
      "http://10.60.110.163:8787/workers\n",
      "http://10.60.110.163:8787/graph\n"
     ]
    }
   ],
   "source": [
    "cluster = SLURMCluster(\n",
    "    cores=10,                          \n",
    "    memory='100GB',  \n",
    "#     memory='1400GB',\n",
    "    \n",
    "    local_directory='~/scratch',\n",
    "    job_extra=[\n",
    "#         '--reservation=lab_rdurante_304',\n",
    "        '--time=05:00:00',\n",
    "        \n",
    "        '--partition=haswell',    \n",
    "        '--nodes=1',\n",
    "        \n",
    "        '--job-name=dask',\n",
    "        '--output=dask.out', \n",
    "        '--error=dask.error', \n",
    "        '--mail-user=aaron.schade@upf.edu',\n",
    "        '--mail-type=NONE', \n",
    "    ],    \n",
    "    n_workers=1,                 # this is internal to one job? one node? \n",
    "    \n",
    "    interface='ib0',               # workers, no diag: em1, em2, ib0,   # no workers: lo, em1.851, idrac, em3 & em4 (no ipv4)\n",
    "    scheduler_options={\n",
    "#         'interface': 'em1',      # it wont allow you specify both an interface AND a host address\n",
    "        'host': '10.30.50.163',    # launch on this address, open dashboard on the other?\n",
    "    },\n",
    ")\n",
    "cluster.scale(jobs=1)\n",
    "\n",
    "\n",
    "scheduler = Client(cluster)\n",
    "print(scheduler)\n",
    "dashboardLink = scheduler.dashboard_link.replace('10.30.50.163', '10.60.110.163')\n",
    "# dashboardLink = scheduler.dashboard_link\n",
    "print(dashboardLink)\n",
    "print(dashboardLink.replace('status', 'workers'))\n",
    "print(dashboardLink.replace('status', 'graph'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\r\n",
      "          19119098   haswell     dask  aschade PD       0:00      1 (None)\r\n"
     ]
    }
   ],
   "source": [
    "!squeue -u aschade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paywalls:\n",
      "{   'azcentral.com': '2012-09-12',\n",
      "    'bostonglobe.com': '2011-10-01',\n",
      "    'chicagotribune.com': '2012-11-01',\n",
      "    'dallasnews.com': '2011-03-01',\n",
      "    'inquirer.com': '2013-12-01',\n",
      "    'latimes.com': '2012-03-05',\n",
      "    'newsday.com': '2009-10-28',\n",
      "    'nj.com': '2020-06-24',\n",
      "    'nypost.com': '2011-06-01',\n",
      "    'nytimes.com': '2011-03-28',\n",
      "    'nytimes.com-1': '2012-04-01',\n",
      "    'nytimes.com-2': '2017-12-01',\n",
      "    'plaindealer.com': '2020-07-01',\n",
      "    'sandiegouniontribune.com': '2012-06-21',\n",
      "    'sfchronicle.com': '2013-03-01',\n",
      "    'staradvertiser.com': '2011-08-03',\n",
      "    'startribune.com': '2011-11-01',\n",
      "    'sun-sentinel.com': '2012-04-09',\n",
      "    'tampabay.com': '2013-09-12',\n",
      "    'washingtonpost.com': '2013-06-12'}\n",
      "\n",
      "news sites:\n",
      "[   '7am.com',\n",
      "    'abcnews.com',\n",
      "    'accessatlanta.com',\n",
      "    'accuweather.com',\n",
      "    'active.com',\n",
      "    'activedayton.com',\n",
      "    'ajc.com',\n",
      "    'ap.org',\n",
      "    'aroundcny.com',\n",
      "    'austin360.com']\n"
     ]
    }
   ],
   "source": [
    "paywallsDF = pd.read_excel('paywalls.xlsx')\n",
    "paywallsDF = paywallsDF[['paywall_date', 'url']]\n",
    "\n",
    "paywalls = {}\n",
    "for index, row in paywallsDF[:18].iterrows():\n",
    "        paywalls[row['url']] = str(row['paywall_date'].date())\n",
    "\n",
    "paywalls['nytimes.com-1'] = '2012-04-01'\n",
    "paywalls['nytimes.com-2'] = '2017-12-01'\n",
    "\n",
    "print('paywalls:')\n",
    "pprint(paywalls, indent=4)\n",
    "\n",
    "######################################\n",
    "\n",
    "with open('newsSitesList.txt', 'r') as f:\n",
    "    newsSitesList = [line.strip() for line in f.readlines()]\n",
    "\n",
    "for site in paywalls.keys():\n",
    "    if site not in newsSitesList:\n",
    "        newsSitesList.append(site) \n",
    "       \n",
    "print('\\nnews sites:')\n",
    "pprint(newsSitesList[:10], indent=4)\n",
    "\n",
    "######################################\n",
    "\n",
    "colsOfInterest = [\n",
    "    # 'ref_domain_name', \n",
    "    'domain_name',\n",
    "    'event_date',\n",
    "\n",
    "    'pages_viewed', \n",
    "    'duration', \n",
    "    # 'event_time', \n",
    "\n",
    "    'hoh_most_education', \n",
    "    'census_region', \n",
    "    'household_size',\n",
    "    'hoh_oldest_age', \n",
    "    'household_income', \n",
    "    'children', \n",
    "    'racial_background',\n",
    "    'connection_speed', \n",
    "    'country_of_origin', \n",
    "    'zip_code', \n",
    "    ]\n",
    "###################\n",
    "\n",
    "individualCharacteristics = [\n",
    "    'hoh_most_education', \n",
    "    'census_region', \n",
    "    'household_size',\n",
    "    'hoh_oldest_age', \n",
    "    'household_income', \n",
    "    'children', \n",
    "    'racial_background',\n",
    "    'connection_speed', \n",
    "    'country_of_origin', \n",
    "    'zip_code', \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for newspaperSite, paywallDate in paywalls.items():\n",
    "    if 'nytimes.com-1' not in newspaperSite: continue\n",
    "    year = int(paywallDate[:4])\n",
    "\n",
    "    ######################################################################################################\n",
    "\n",
    "    print(f'\\n' + f' {newspaperSite} '.center(80, '#'))\n",
    "\n",
    "    prePWmonths = pd.Timestamp(paywallDate).dayofyear/30.5\n",
    "    postPWmonths = 12 - prePWmonths\n",
    "    if prePWmonths < 1 or postPWmonths < 1: continue\n",
    "\n",
    "    idealRangeStart = pd.Timestamp(paywallDate) - pd.Timedelta('90 days')\n",
    "    idealRangeEnd = pd.Timestamp(paywallDate) + pd.Timedelta('90 days')\n",
    "    yearStart = pd.Timestamp(f'{year}-01-01')\n",
    "    yearEnd = pd.Timestamp(f'{year}-12-31')\n",
    "\n",
    "    rangeStart = floor(idealRangeStart, yearStart)\n",
    "    rangeEnd = ceiling(idealRangeEnd, yearEnd)\n",
    "    datesOfInterest = pd.date_range(rangeStart, rangeEnd, freq='W')\n",
    "\n",
    "    print(f'paywall date: {paywallDate}')\n",
    "    print(list(datesOfInterest.month.unique()))\n",
    "\n",
    "    ######################################################################################################\n",
    "\n",
    "    ddf = dd.read_parquet(\n",
    "        f'../comscore/parquet/{year}', \n",
    "        index='machine_id', \n",
    "        columns=colsOfInterest,\n",
    "        engine='fastparquet',   # you HAVE TO use the same engine to read as you did for creating the parquet files!!!  only then will fast index lookups work - however, you need the pyarrow (or python-snappy) package installed for google's amazing 'snappy' compression algo\n",
    "        )\n",
    "    ddf = ddf[ddf['event_date'].between(rangeStart, rangeEnd)]\n",
    "\n",
    "    visitors = list(\n",
    "        ddf.loc[ddf['domain_name'] == newspaperSite.strip('-12')]\n",
    "        .index\n",
    "        .unique()\n",
    "        .compute()\n",
    "        )\n",
    "    # visitors = [13512886]\n",
    "\n",
    "\n",
    "    for visitor in tqdm(visitors, desc='visitors done: '):\n",
    "        thisVisitorRows = []\n",
    "\n",
    "\n",
    "        # load into memory\n",
    "        df = ddf.loc[visitor].compute()\n",
    "        df['numberVisits'] = 1           # fill all rows with 1 -> when aggregated, becomes counter\n",
    "\n",
    "        # get all websites they visited\n",
    "        allSites = df['domain_name'].unique()\n",
    "\n",
    "        # setup one row with default values\n",
    "        defaultValues = {\n",
    "            'machine_id':           visitor, \n",
    "            'date':                 '', \n",
    "            'day_of_month':         '', \n",
    "            'week_of_month':        '', \n",
    "            'month':                '', \n",
    "            'year':                 '', \n",
    "\n",
    "            'domain_name':          '', \n",
    "            'news_site_dummy':      0, \n",
    "            'number_visits':        0, \n",
    "            'number_pages_viewed':  0, \n",
    "            'time_spent_on_site':   0, \n",
    "            }\n",
    "        # add individual characteristics\n",
    "        defaultValues.update(df.loc[visitor, individualCharacteristics].iloc[0].to_dict())     \n",
    "\n",
    "\n",
    "\n",
    "        # aggregate by day and website, do not keep individual characteristics\n",
    "        notIndividualCharacteristics = [col for col in df.columns if col not in individualCharacteristics]\n",
    "        agged = (\n",
    "            df[notIndividualCharacteristics]\n",
    "            .groupby([pd.Grouper(key='event_date', freq='W', ), 'domain_name'])\n",
    "            .sum()\n",
    "            .reset_index(drop=False)\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # were in one individual, \n",
    "        # for this individual, for each date, for each site; \n",
    "        # fill in the information you have, if not, leave default\n",
    "        for date in datesOfInterest:\n",
    "            for site in allSites:\n",
    "\n",
    "                # identifying stuff\n",
    "                thisRow = defaultValues.copy()\n",
    "                # date = pd.Timestamp(date)\n",
    "                thisRow['date'] = date.strftime('%Y-%m-%d')\n",
    "                thisRow['day_of_month'] = date.day\n",
    "                thisRow['week_of_month'] = (date.day - 1)//7 + 1\n",
    "                thisRow['month'] = date.month\n",
    "                thisRow['year'] = date.year\n",
    "\n",
    "                thisRow['domain_name'] = site\n",
    "\n",
    "                thisRow['news_site_dummy'] = 1 if site in newsSitesList else 0\n",
    "\n",
    "\n",
    "                # aggregation information\n",
    "                aggedRowOfInterest = agged.loc[(agged['event_date'] == date) & (agged['domain_name'] == site)]\n",
    "\n",
    "                if len(aggedRowOfInterest) != 0:\n",
    "                    thisRow['number_visits'] = aggedRowOfInterest['numberVisits'].values[0]\n",
    "                    thisRow['number_pages_viewed'] = aggedRowOfInterest['pages_viewed'].values[0]\n",
    "                    thisRow['time_spent_on_site'] = aggedRowOfInterest['duration'].values[0]\n",
    "\n",
    "\n",
    "\n",
    "                thisVisitorRows.append(thisRow)\n",
    "\n",
    "\n",
    "\n",
    "        # printing to file\n",
    "        # one per individual\n",
    "        subfolder = f'outputs/longTable/{newspaperSite}/individuals'\n",
    "        if not exists(subfolder): makedirs(subfolder)\n",
    "\n",
    "        with open(f'{subfolder}/{visitor}.csv', 'w', newline='') as file:\n",
    "            keys = thisVisitorRows[0].keys()\n",
    "            dict_writer = csv.DictWriter(file, keys)\n",
    "\n",
    "            dict_writer.writeheader()\n",
    "            dict_writer.writerows(thisVisitorRows)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ddf = (\n",
    "        dd.read_csv(\n",
    "            f'{subfolder}/*.csv', \n",
    "            encoding_errors='replace')\n",
    "        .set_index(\n",
    "            'machine_id', \n",
    "            npartitions='auto', \n",
    "            compute=False)\n",
    "    )\n",
    "\n",
    "\n",
    "    ddf.to_csv(f'outputs/longTable/{newspaperSite}/long_table.csv', single_file=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/gpfs42/robbyfs/scratch/lab_rdurante/aschade/paywalls/outputs/longTable/nytimes.com-1/long_table.csv']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newspaperSite = 'nytimes.com-1'\n",
    "subfolder = f'outputs/longTable/{newspaperSite}/individuals'\n",
    "ddf = dd.read_csv(\n",
    "        f'{subfolder}/*.csv', \n",
    "        encoding_errors='replace', \n",
    ")\n",
    "\n",
    "\n",
    "ddf.to_csv(f'outputs/longTable/{newspaperSite}/long_table.csv', single_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>machine_id</th>\n",
       "      <th>date</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>week_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>domain_name</th>\n",
       "      <th>news_site_dummy</th>\n",
       "      <th>number_visits</th>\n",
       "      <th>number_pages_viewed</th>\n",
       "      <th>time_spent_on_site</th>\n",
       "      <th>hoh_most_education</th>\n",
       "      <th>census_region</th>\n",
       "      <th>household_size</th>\n",
       "      <th>hoh_oldest_age</th>\n",
       "      <th>household_income</th>\n",
       "      <th>children</th>\n",
       "      <th>racial_background</th>\n",
       "      <th>connection_speed</th>\n",
       "      <th>country_of_origin</th>\n",
       "      <th>zip_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>99534294</td>\n",
       "      <td>2017-09-10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2017</td>\n",
       "      <td>envybox.io</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>99534294</td>\n",
       "      <td>2017-10-15</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>dom.co.il</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>99534294</td>\n",
       "      <td>2017-10-15</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>graniru.org</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>99534294</td>\n",
       "      <td>2017-10-15</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>yandex.net</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>99534294</td>\n",
       "      <td>2017-10-15</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2017</td>\n",
       "      <td>likebtn.com</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   machine_id        date  day_of_month  week_of_month  month  year  \\\n",
       "0    99534294  2017-09-10            10              2      9  2017   \n",
       "1    99534294  2017-10-15            15              3     10  2017   \n",
       "2    99534294  2017-10-15            15              3     10  2017   \n",
       "3    99534294  2017-10-15            15              3     10  2017   \n",
       "4    99534294  2017-10-15            15              3     10  2017   \n",
       "\n",
       "   domain_name  news_site_dummy  number_visits  number_pages_viewed  \\\n",
       "0   envybox.io                0              1                    1   \n",
       "1    dom.co.il                0              1                    1   \n",
       "2  graniru.org                0              0                    0   \n",
       "3   yandex.net                0              1                    1   \n",
       "4  likebtn.com                0              1                    1   \n",
       "\n",
       "   time_spent_on_site  hoh_most_education  census_region  household_size  \\\n",
       "0                   1                  99              3               1   \n",
       "1                   1                  99              3               1   \n",
       "2                   0                  99              3               1   \n",
       "3                   1                  99              3               1   \n",
       "4                   1                  99              3               1   \n",
       "\n",
       "   hoh_oldest_age  household_income  children  racial_background  \\\n",
       "0              11                16         0                  1   \n",
       "1              11                16         0                  1   \n",
       "2              11                16         0                  1   \n",
       "3              11                16         0                  1   \n",
       "4              11                16         0                  1   \n",
       "\n",
       "   connection_speed  country_of_origin  zip_code  \n",
       "0                 1                  0     77084  \n",
       "1                 1                  0     77084  \n",
       "2                 1                  0     77084  \n",
       "3                 1                  0     77084  \n",
       "4                 1                  0     77084  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "########################################## testing ####################################\n",
    "\n",
    "ddf = dd.read_csv('outputs/longTable/nytimes.com-2/long_table.csv')\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n",
      "Columns: 21 entries, machine_id to zip_code\n",
      "dtypes: object(2), int64(19)"
     ]
    }
   ],
   "source": [
    "ddf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49370796"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8379"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.machine_id.nunique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
